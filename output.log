nohup: ignoring input
Matplotlib is building the font cache; this may take a moment.
[92mINFO [0m:      Starting Flower simulation, config: num_rounds=5, no round_timeout
2024-10-10 23:19:41,200	INFO worker.py:1752 -- Started a local Ray instance.
[92mINFO [0m:      Flower VCE: Ray initialized with resources: {'node:10.0.0.4': 1.0, 'node:__internal_head__': 1.0, 'memory': 3709285172.0, 'object_store_memory': 1854642585.0, 'CPU': 2.0}
[92mINFO [0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html
[92mINFO [0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}
[92mINFO [0m:      Flower VCE: Creating VirtualClientEngineActorPool with 2 actors
[92mINFO [0m:      [INIT]
[92mINFO [0m:      Requesting initial parameters from one random client
[92mINFO [0m:      Received initial parameters from one random client
[92mINFO [0m:      Starting evaluation of initial global parameters
[92mINFO [0m:      Evaluation returned no results (`None`)
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 1]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[36m(ClientAppActor pid=13851)[0m /home/azureuser/FL/vae.py:97: UserWarning: Using a target size (torch.Size([1, 10, 10])) that is different to the input size (torch.Size([10, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
[36m(ClientAppActor pid=13851)[0m   recon_loss = F.mse_loss(recon_x, x, reduction='mean')
[36m(ClientAppActor pid=13851)[0m Epoch:[1/100] Train loss: 0.051165404607821256, Recon loss: 0.04151536303361063, KL Divergence: 1.2813893797121942
[36m(ClientAppActor pid=13851)[0m Epoch:[2/100] Train loss: 0.01957205346566625, Recon loss: 0.013110188200691483, KL Divergence: 1.8523458444714547[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[36m(ClientAppActor pid=13851)[0m Epoch:[3/100] Train loss: 0.013941751221031883, Recon loss: 0.010043150498662726, KL Divergence: 2.231967393016815[32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=13850)[0m Epoch:[4/100] Train loss: 0.010944092956534587, Recon loss: 0.008652668901102152, KL Divergence: 2.620689665746689[32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=13850)[0m Epoch:[5/100] Train loss: 0.009374779412895442, Recon loss: 0.008063159072762937, KL Divergence: 2.9901707568407057[32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=13850)[0m Epoch:[6/100] Train loss: 0.008508728893171065, Recon loss: 0.007768808406328753, KL Divergence: 3.355601425123215[32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=13850)[0m Epoch:[7/100] Train loss: 0.00800488906998653, Recon loss: 0.007592222028553078, KL Divergence: 3.7110413556575774[32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=13851)[0m Epoch:[7/100] Train loss: 0.007931213580543408, Recon loss: 0.0075180185913817695, KL Divergence: 3.734309333920479
[36m(ClientAppActor pid=13850)[0m Epoch:[8/100] Train loss: 0.007702766020869604, Recon loss: 0.007474374163357061, KL Divergence: 4.054795269107818
[36m(ClientAppActor pid=13851)[0m Epoch:[8/100] Train loss: 0.007653371400662581, Recon loss: 0.007425370271237625, KL Divergence: 4.0829228039979935
[36m(ClientAppActor pid=13850)[0m Epoch:[9/100] Train loss: 0.007504276180319721, Recon loss: 0.007378377098617966, KL Divergence: 4.366431424665451
[36m(ClientAppActor pid=13851)[0m Epoch:[9/100] Train loss: 0.007433150495187147, Recon loss: 0.007308802445479524, KL Divergence: 4.379275434041023
[36m(ClientAppActor pid=13850)[0m Epoch:[10/100] Train loss: 0.007378470363164524, Recon loss: 0.007308649892348512, KL Divergence: 4.640984963774681
[36m(ClientAppActor pid=13851)[0m Epoch:[10/100] Train loss: 0.00731031318575624, Recon loss: 0.007242509666027036, KL Divergence: 4.641339036393165
[36m(ClientAppActor pid=13850)[0m Epoch:[11/100] Train loss: 0.00728729539308697, Recon loss: 0.007248210566544731, KL Divergence: 4.857233374428749
[36m(ClientAppActor pid=13851)[0m Epoch:[11/100] Train loss: 0.007213861061298303, Recon loss: 0.0071763985457775565, KL Divergence: 4.877526555275917
[36m(ClientAppActor pid=13850)[0m Epoch:[12/100] Train loss: 0.00722863922036413, Recon loss: 0.007206013139526385, KL Divergence: 5.0210199896097185
[36m(ClientAppActor pid=13851)[0m Epoch:[12/100] Train loss: 0.007158073654835243, Recon loss: 0.007136841677469602, KL Divergence: 5.039799942088127
[36m(ClientAppActor pid=13850)[0m Epoch:[13/100] Train loss: 0.007179233806530283, Recon loss: 0.007165382217596471, KL Divergence: 5.14129416706562
[36m(ClientAppActor pid=13851)[0m Epoch:[13/100] Train loss: 0.00709280774322433, Recon loss: 0.0070802006328896825, KL Divergence: 5.122938256359101
[36m(ClientAppActor pid=13850)[0m Epoch:[14/100] Train loss: 0.00713416957874997, Recon loss: 0.007124960095072674, KL Divergence: 5.227962323713303
[36m(ClientAppActor pid=13851)[0m Epoch:[14/100] Train loss: 0.007060830936773345, Recon loss: 0.0070528541860796394, KL Divergence: 5.2121256126403805
[36m(ClientAppActor pid=13850)[0m Epoch:[15/100] Train loss: 0.00710133868547, Recon loss: 0.0070946860429338815, KL Divergence: 5.278644877910614
[36m(ClientAppActor pid=13850)[0m Epoch:[16/100] Train loss: 0.007068572290250086, Recon loss: 0.007063278180608814, KL Divergence: 5.300491374635697[32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=13851)[0m Epoch:[17/100] Train loss: 0.006964157236131541, Recon loss: 0.00696031127897495, KL Divergence: 5.347099562168121[32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=13850)[0m Epoch:[18/100] Train loss: 0.007015977479787762, Recon loss: 0.007011981877079051, KL Divergence: 5.354721060085296[32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=13851)[0m Epoch:[19/100] Train loss: 0.006916186680157807, Recon loss: 0.006912958897014869, KL Divergence: 5.358654150533676[32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=13850)[0m Epoch:[19/100] Train loss: 0.007000269075164761, Recon loss: 0.006996553010181833, KL Divergence: 5.368840775108337
[36m(ClientAppActor pid=13851)[0m Epoch:[20/100] Train loss: 0.006899564396852475, Recon loss: 0.006896420792260687, KL Divergence: 5.398415492963791
[36m(ClientAppActor pid=13850)[0m Epoch:[20/100] Train loss: 0.006977040860870238, Recon loss: 0.006973621688333332, KL Divergence: 5.400021326947212
[36m(ClientAppActor pid=13851)[0m Epoch:[21/100] Train loss: 0.006876887121632353, Recon loss: 0.006873881834502208, KL Divergence: 5.408005271100998
[36m(ClientAppActor pid=13850)[0m Epoch:[21/100] Train loss: 0.006969228262018351, Recon loss: 0.006965989886861644, KL Divergence: 5.39811337082386
[36m(ClientAppActor pid=13851)[0m Epoch:[22/100] Train loss: 0.00686315931284953, Recon loss: 0.006860226679656444, KL Divergence: 5.408810545420646
[36m(ClientAppActor pid=13850)[0m Epoch:[22/100] Train loss: 0.006940844732901496, Recon loss: 0.006937706633960807, KL Divergence: 5.404218194723129
[36m(ClientAppActor pid=13851)[0m Epoch:[23/100] Train loss: 0.0068451791557679826, Recon loss: 0.006842269232685066, KL Divergence: 5.41798653113842
[36m(ClientAppActor pid=13850)[0m Epoch:[23/100] Train loss: 0.006927528969815375, Recon loss: 0.006924514145594185, KL Divergence: 5.4431884307384495
[36m(ClientAppActor pid=13851)[0m Epoch:[24/100] Train loss: 0.006823118511756002, Recon loss: 0.006820338240142337, KL Divergence: 5.4537403756380085
[36m(ClientAppActor pid=13850)[0m Epoch:[24/100] Train loss: 0.006914555717654639, Recon loss: 0.006911627859179862, KL Divergence: 5.455749546933174
[36m(ClientAppActor pid=13851)[0m Epoch:[25/100] Train loss: 0.006812241322276532, Recon loss: 0.006809480426201117, KL Divergence: 5.435504137754441
[36m(ClientAppActor pid=13850)[0m Epoch:[25/100] Train loss: 0.006905907371936246, Recon loss: 0.006903063883759478, KL Divergence: 5.48343279607296
[36m(ClientAppActor pid=13851)[0m Epoch:[26/100] Train loss: 0.00679642503768755, Recon loss: 0.006793797047747466, KL Divergence: 5.438885789656639
[36m(ClientAppActor pid=13850)[0m Epoch:[26/100] Train loss: 0.0068885034446965615, Recon loss: 0.006885732840092532, KL Divergence: 5.496116639280319
[36m(ClientAppActor pid=13851)[0m Epoch:[27/100] Train loss: 0.006779710567730217, Recon loss: 0.006777126308806146, KL Divergence: 5.479309787011147
[36m(ClientAppActor pid=13850)[0m Epoch:[27/100] Train loss: 0.0068713621302224056, Recon loss: 0.006868637112453962, KL Divergence: 5.468940038752556
[36m(ClientAppActor pid=13851)[0m Epoch:[28/100] Train loss: 0.00676585042786628, Recon loss: 0.006763392206256503, KL Divergence: 5.466134282422066
[36m(ClientAppActor pid=13850)[0m Epoch:[28/100] Train loss: 0.006854834302515519, Recon loss: 0.006852167893834394, KL Divergence: 5.51325993514061
[36m(ClientAppActor pid=13851)[0m Epoch:[29/100] Train loss: 0.006758761372858862, Recon loss: 0.006756259863472405, KL Divergence: 5.502788586521149
[36m(ClientAppActor pid=13850)[0m Epoch:[29/100] Train loss: 0.006850979125909543, Recon loss: 0.006848334746514319, KL Divergence: 5.5158794612646105
[36m(ClientAppActor pid=13851)[0m Epoch:[30/100] Train loss: 0.006742014050597209, Recon loss: 0.006739637814530033, KL Divergence: 5.480347044372558
[36m(ClientAppActor pid=13850)[0m Epoch:[30/100] Train loss: 0.006837548240516207, Recon loss: 0.00683500883746783, KL Divergence: 5.545634267473221
[36m(ClientAppActor pid=13851)[0m Epoch:[31/100] Train loss: 0.0067268316069521465, Recon loss: 0.00672445612981628, KL Divergence: 5.509760607695579
[36m(ClientAppActor pid=13850)[0m Epoch:[31/100] Train loss: 0.00682826406427248, Recon loss: 0.006825668387926453, KL Divergence: 5.533070675373077
[36m(ClientAppActor pid=13851)[0m Epoch:[32/100] Train loss: 0.0067213840124442864, Recon loss: 0.00671900167469903, KL Divergence: 5.530249158763885
[36m(ClientAppActor pid=13850)[0m Epoch:[32/100] Train loss: 0.006816851711283471, Recon loss: 0.006814337396183691, KL Divergence: 5.543402164006233
[36m(ClientAppActor pid=13851)[0m Epoch:[33/100] Train loss: 0.006706667924005888, Recon loss: 0.006704424076945725, KL Divergence: 5.533236837482453
[36m(ClientAppActor pid=13850)[0m Epoch:[33/100] Train loss: 0.006814940933600792, Recon loss: 0.00681244156251978, KL Divergence: 5.544595622825622
[36m(ClientAppActor pid=13851)[0m Epoch:[34/100] Train loss: 0.006701793587607062, Recon loss: 0.006699552213405332, KL Divergence: 5.552727620983124
[36m(ClientAppActor pid=13850)[0m Epoch:[34/100] Train loss: 0.006801313859168112, Recon loss: 0.00679885560944731, KL Divergence: 5.571499420714378
[36m(ClientAppActor pid=13851)[0m Epoch:[35/100] Train loss: 0.006689518670616599, Recon loss: 0.006687245766416708, KL Divergence: 5.556699772191048
[36m(ClientAppActor pid=13850)[0m Epoch:[35/100] Train loss: 0.006794321705300899, Recon loss: 0.006791902082255729, KL Divergence: 5.594140004634857
[36m(ClientAppActor pid=13851)[0m Epoch:[36/100] Train loss: 0.00668099023912564, Recon loss: 0.006678832600053102, KL Divergence: 5.557117528367042
[36m(ClientAppActor pid=13850)[0m Epoch:[36/100] Train loss: 0.0067802049061491745, Recon loss: 0.006777843760950145, KL Divergence: 5.589906918001175
[36m(ClientAppActor pid=13851)[0m Epoch:[37/100] Train loss: 0.006669188551893785, Recon loss: 0.00666697493724605, KL Divergence: 5.5724674693107605
[36m(ClientAppActor pid=13850)[0m Epoch:[37/100] Train loss: 0.006772660941018694, Recon loss: 0.006770265612894036, KL Divergence: 5.602239734482765
[36m(ClientAppActor pid=13851)[0m Epoch:[38/100] Train loss: 0.0066641766512770114, Recon loss: 0.00666199623899829, KL Divergence: 5.594763733291626
[36m(ClientAppActor pid=13850)[0m Epoch:[38/100] Train loss: 0.006767704780604072, Recon loss: 0.006765354277073584, KL Divergence: 5.58784064309597
[36m(ClientAppActor pid=13851)[0m Epoch:[39/100] Train loss: 0.006653014260549753, Recon loss: 0.006650845776133247, KL Divergence: 5.583909311008453
[36m(ClientAppActor pid=13850)[0m Epoch:[39/100] Train loss: 0.006757104072114271, Recon loss: 0.0067547564706656885, KL Divergence: 5.607233030557633
[36m(ClientAppActor pid=13851)[0m Epoch:[40/100] Train loss: 0.006646625418846998, Recon loss: 0.006644506932687727, KL Divergence: 5.5962029891729355
[36m(ClientAppActor pid=13850)[0m Epoch:[40/100] Train loss: 0.00675152736320033, Recon loss: 0.006749237212787739, KL Divergence: 5.639888150453568
[36m(ClientAppActor pid=13851)[0m Epoch:[41/100] Train loss: 0.00663925443831049, Recon loss: 0.006637080308684017, KL Divergence: 5.602965372943878
[36m(ClientAppActor pid=13850)[0m Epoch:[41/100] Train loss: 0.006748656650230532, Recon loss: 0.006746312323541042, KL Divergence: 5.662634776067733
[36m(ClientAppActor pid=13851)[0m Epoch:[42/100] Train loss: 0.006630401620834709, Recon loss: 0.006628261585731889, KL Divergence: 5.613050933170318
[36m(ClientAppActor pid=13850)[0m Epoch:[42/100] Train loss: 0.006734660731046188, Recon loss: 0.006732373778728379, KL Divergence: 5.662424029302597
[36m(ClientAppActor pid=13851)[0m Epoch:[43/100] Train loss: 0.006621323890729946, Recon loss: 0.006619215818027623, KL Divergence: 5.653753041815758
[36m(ClientAppActor pid=13850)[0m Epoch:[43/100] Train loss: 0.006726615573971776, Recon loss: 0.006724384310536061, KL Divergence: 5.666946313929558
[36m(ClientAppActor pid=13851)[0m Epoch:[44/100] Train loss: 0.00661454068934363, Recon loss: 0.0066124803693111064, KL Divergence: 5.6202248007535935
[36m(ClientAppActor pid=13850)[0m Epoch:[44/100] Train loss: 0.006723870793393962, Recon loss: 0.006721625467948343, KL Divergence: 5.695860664510727
[36m(ClientAppActor pid=13851)[0m Epoch:[45/100] Train loss: 0.006606683337428058, Recon loss: 0.006604649983881336, KL Divergence: 5.666635234284401
[36m(ClientAppActor pid=13850)[0m Epoch:[45/100] Train loss: 0.006717543719804689, Recon loss: 0.006715365589710382, KL Divergence: 5.659055203461647
[36m(ClientAppActor pid=13851)[0m Epoch:[46/100] Train loss: 0.006603499202427338, Recon loss: 0.006601401912338224, KL Divergence: 5.65066010415554
[36m(ClientAppActor pid=13850)[0m Epoch:[46/100] Train loss: 0.006705718081751183, Recon loss: 0.006703513462408501, KL Divergence: 5.661563810420036
[36m(ClientAppActor pid=13851)[0m Epoch:[47/100] Train loss: 0.006595501976058586, Recon loss: 0.00659344961525785, KL Divergence: 5.663928203248978
[36m(ClientAppActor pid=13850)[0m Epoch:[47/100] Train loss: 0.0067057682149251376, Recon loss: 0.006703526919285923, KL Divergence: 5.688564544796944
[36m(ClientAppActor pid=13851)[0m Epoch:[48/100] Train loss: 0.006585857239415782, Recon loss: 0.006583788691292284, KL Divergence: 5.6782369288682935
[36m(ClientAppActor pid=13850)[0m Epoch:[48/100] Train loss: 0.006699586167618236, Recon loss: 0.006697380299579254, KL Divergence: 5.721316361999512
[36m(ClientAppActor pid=13851)[0m Epoch:[49/100] Train loss: 0.006581259675564798, Recon loss: 0.006579209725575856, KL Divergence: 5.678063300418854
[36m(ClientAppActor pid=13850)[0m Epoch:[49/100] Train loss: 0.0066936728766794655, Recon loss: 0.006691437036005209, KL Divergence: 5.71840400800705
[36m(ClientAppActor pid=13851)[0m Epoch:[50/100] Train loss: 0.006573369312437717, Recon loss: 0.006571357052325948, KL Divergence: 5.685956129622459
[36m(ClientAppActor pid=13850)[0m Epoch:[50/100] Train loss: 0.006682962413123732, Recon loss: 0.006680794068252544, KL Divergence: 5.71531125125885
[36m(ClientAppActor pid=13851)[0m Epoch:[51/100] Train loss: 0.006570128670196846, Recon loss: 0.006568099531486678, KL Divergence: 5.662659595727921
[36m(ClientAppActor pid=13850)[0m Epoch:[51/100] Train loss: 0.006679688686279951, Recon loss: 0.006677526915101134, KL Divergence: 5.710236330389977
[36m(ClientAppActor pid=13851)[0m Epoch:[52/100] Train loss: 0.0065671663477831314, Recon loss: 0.006565133471848458, KL Divergence: 5.699631410980224
[36m(ClientAppActor pid=13850)[0m Epoch:[52/100] Train loss: 0.006674946136844846, Recon loss: 0.006672773376755959, KL Divergence: 5.73336151292324
[36m(ClientAppActor pid=13851)[0m Epoch:[53/100] Train loss: 0.006556735715676587, Recon loss: 0.006554739940169202, KL Divergence: 5.703501975631714
[36m(ClientAppActor pid=13850)[0m Epoch:[53/100] Train loss: 0.006667610672260435, Recon loss: 0.00666548208157601, KL Divergence: 5.733880789065361
[36m(ClientAppActor pid=13851)[0m Epoch:[54/100] Train loss: 0.006553926821373625, Recon loss: 0.006551974672421511, KL Divergence: 5.741702564072609
[36m(ClientAppActor pid=13850)[0m Epoch:[54/100] Train loss: 0.006666019053024865, Recon loss: 0.006663839693282716, KL Divergence: 5.732267051839829
[36m(ClientAppActor pid=13851)[0m Epoch:[55/100] Train loss: 0.006542901729757523, Recon loss: 0.006540953339225689, KL Divergence: 5.724467202353478
[36m(ClientAppActor pid=13850)[0m Epoch:[55/100] Train loss: 0.006659898788005649, Recon loss: 0.006657762497528438, KL Divergence: 5.757893241167069
[36m(ClientAppActor pid=13851)[0m Epoch:[56/100] Train loss: 0.006542138877348134, Recon loss: 0.0065401746272384115, KL Divergence: 5.737199854516983
[36m(ClientAppActor pid=13851)[0m Epoch:[57/100] Train loss: 0.006536349855861045, Recon loss: 0.006534328653344346, KL Divergence: 5.7452266568899155
[36m(ClientAppActor pid=13850)[0m Epoch:[56/100] Train loss: 0.006656251365820481, Recon loss: 0.006654145847115433, KL Divergence: 5.7661194338560104
[36m(ClientAppActor pid=13851)[0m Epoch:[58/100] Train loss: 0.006531978693452766, Recon loss: 0.006529971023977032, KL Divergence: 5.747425836253166
[36m(ClientAppActor pid=13850)[0m Epoch:[57/100] Train loss: 0.006648432317289826, Recon loss: 0.0066462890934426465, KL Divergence: 5.758044474315644
[36m(ClientAppActor pid=13851)[0m Epoch:[59/100] Train loss: 0.006527524340936725, Recon loss: 0.006525493265184923, KL Divergence: 5.742027515411377
[36m(ClientAppActor pid=13850)[0m Epoch:[58/100] Train loss: 0.006644916780454787, Recon loss: 0.006642802987807772, KL Divergence: 5.757933280467987
[36m(ClientAppActor pid=13851)[0m Epoch:[60/100] Train loss: 0.006521767567531424, Recon loss: 0.00651977924372386, KL Divergence: 5.748102617502212
[36m(ClientAppActor pid=13850)[0m Epoch:[59/100] Train loss: 0.006645108419187454, Recon loss: 0.0066430037748010365, KL Divergence: 5.738735858511925
[36m(ClientAppActor pid=13851)[0m Epoch:[61/100] Train loss: 0.006519473222074976, Recon loss: 0.006517523407717362, KL Divergence: 5.765566203165054
[36m(ClientAppActor pid=13850)[0m Epoch:[60/100] Train loss: 0.006637466572528865, Recon loss: 0.006635367211338053, KL Divergence: 5.780780832815171
[36m(ClientAppActor pid=13851)[0m Epoch:[62/100] Train loss: 0.006513210415955291, Recon loss: 0.006511197005623217, KL Divergence: 5.778965600132942
[36m(ClientAppActor pid=13850)[0m Epoch:[61/100] Train loss: 0.006639368598369947, Recon loss: 0.006637239887526084, KL Divergence: 5.785818196201324
[36m(ClientAppActor pid=13851)[0m Epoch:[63/100] Train loss: 0.006508718938779021, Recon loss: 0.0065067448775745105, KL Divergence: 5.774064919567108
[36m(ClientAppActor pid=13850)[0m Epoch:[62/100] Train loss: 0.0066290458020415825, Recon loss: 0.006626951814448239, KL Divergence: 5.7683637514591215
[36m(ClientAppActor pid=13851)[0m Epoch:[64/100] Train loss: 0.006505333866349065, Recon loss: 0.006503327258819627, KL Divergence: 5.773716915631295
[36m(ClientAppActor pid=13850)[0m Epoch:[63/100] Train loss: 0.0066250446470768115, Recon loss: 0.006622966866423121, KL Divergence: 5.796391197347641
[36m(ClientAppActor pid=13851)[0m Epoch:[65/100] Train loss: 0.006501385276154179, Recon loss: 0.006499356415901275, KL Divergence: 5.770544174838066
[36m(ClientAppActor pid=13850)[0m Epoch:[64/100] Train loss: 0.006619232025424936, Recon loss: 0.006617133722718791, KL Divergence: 5.797138317370415
[36m(ClientAppActor pid=13851)[0m Epoch:[66/100] Train loss: 0.0064977844127755814, Recon loss: 0.006495740244935586, KL Divergence: 5.802914033389092
[36m(ClientAppActor pid=13850)[0m Epoch:[65/100] Train loss: 0.0066173756064244115, Recon loss: 0.006615273013544356, KL Divergence: 5.807929100513459
[36m(ClientAppActor pid=13851)[0m Epoch:[67/100] Train loss: 0.006495447555775263, Recon loss: 0.006493411510886562, KL Divergence: 5.809587744355202
[36m(ClientAppActor pid=13850)[0m Epoch:[66/100] Train loss: 0.006618845296632935, Recon loss: 0.00661674675673039, KL Divergence: 5.793861792755127
[36m(ClientAppActor pid=13851)[0m Epoch:[68/100] Train loss: 0.0064911633527579395, Recon loss: 0.006489119274095629, KL Divergence: 5.810064294052124
[36m(ClientAppActor pid=13850)[0m Epoch:[67/100] Train loss: 0.006614008436203494, Recon loss: 0.006611941569584451, KL Divergence: 5.794489949035644
[36m(ClientAppActor pid=13851)[0m Epoch:[69/100] Train loss: 0.006486516943350853, Recon loss: 0.00648446733637802, KL Divergence: 5.815227909016609
[36m(ClientAppActor pid=13850)[0m Epoch:[68/100] Train loss: 0.006607630100273764, Recon loss: 0.0066055377923175005, KL Divergence: 5.796301227593422
[36m(ClientAppActor pid=13851)[0m Epoch:[70/100] Train loss: 0.006485436626807222, Recon loss: 0.0064833725950169535, KL Divergence: 5.790271952390671
[36m(ClientAppActor pid=13850)[0m Epoch:[69/100] Train loss: 0.0066065237201026325, Recon loss: 0.006604394305024198, KL Divergence: 5.80750881152153
[36m(ClientAppActor pid=13851)[0m Epoch:[71/100] Train loss: 0.006478672712835487, Recon loss: 0.0064766681237775625, KL Divergence: 5.802800653219223
[36m(ClientAppActor pid=13850)[0m Epoch:[70/100] Train loss: 0.006604065918951983, Recon loss: 0.006601958808858763, KL Divergence: 5.832630670428276
[36m(ClientAppActor pid=13851)[0m Epoch:[72/100] Train loss: 0.006477693593380082, Recon loss: 0.00647563539538105, KL Divergence: 5.854901163291931
[36m(ClientAppActor pid=13850)[0m Epoch:[71/100] Train loss: 0.006599128531374481, Recon loss: 0.006597044694992746, KL Divergence: 5.835264898228646
[36m(ClientAppActor pid=13851)[0m Epoch:[73/100] Train loss: 0.006475275483237511, Recon loss: 0.006473217780046889, KL Divergence: 5.8368942088603974
[36m(ClientAppActor pid=13850)[0m Epoch:[72/100] Train loss: 0.006594673718612103, Recon loss: 0.006592602054453073, KL Divergence: 5.809047545099259
[36m(ClientAppActor pid=13851)[0m Epoch:[74/100] Train loss: 0.006468735369156002, Recon loss: 0.006466693477656554, KL Divergence: 5.832181549954415
[36m(ClientAppActor pid=13850)[0m Epoch:[73/100] Train loss: 0.006594918452869479, Recon loss: 0.006592812640556167, KL Divergence: 5.825248841357231
[36m(ClientAppActor pid=13851)[0m Epoch:[75/100] Train loss: 0.006468599564745091, Recon loss: 0.0064665469404733816, KL Divergence: 5.842276661729812
[36m(ClientAppActor pid=13850)[0m Epoch:[74/100] Train loss: 0.006588872787316268, Recon loss: 0.0065867605674163315, KL Divergence: 5.844033105516433
[36m(ClientAppActor pid=13851)[0m Epoch:[76/100] Train loss: 0.006462354766005865, Recon loss: 0.006460304943213304, KL Divergence: 5.838051846265793
[36m(ClientAppActor pid=13850)[0m Epoch:[75/100] Train loss: 0.00658684487146229, Recon loss: 0.006584738651378393, KL Divergence: 5.83155106048584
[36m(ClientAppActor pid=13851)[0m Epoch:[77/100] Train loss: 0.006462283691771609, Recon loss: 0.006460217145310889, KL Divergence: 5.852249066925049
[36m(ClientAppActor pid=13850)[0m Epoch:[76/100] Train loss: 0.006584688690504663, Recon loss: 0.0065826003581906664, KL Divergence: 5.8277730884075165
[36m(ClientAppActor pid=13851)[0m Epoch:[78/100] Train loss: 0.006459332338682362, Recon loss: 0.006457240008330064, KL Divergence: 5.853479208397865
[36m(ClientAppActor pid=13850)[0m Epoch:[77/100] Train loss: 0.0065838296730845285, Recon loss: 0.006581694508143482, KL Divergence: 5.874516859388351
[36m(ClientAppActor pid=13851)[0m Epoch:[79/100] Train loss: 0.006457497922550283, Recon loss: 0.006455396952999399, KL Divergence: 5.842639249444008
[36m(ClientAppActor pid=13850)[0m Epoch:[78/100] Train loss: 0.006581019435066355, Recon loss: 0.006578864526579128, KL Divergence: 5.859445439815521
[36m(ClientAppActor pid=13851)[0m Epoch:[80/100] Train loss: 0.006453749876507754, Recon loss: 0.006451620408559393, KL Divergence: 5.864582427430153
[36m(ClientAppActor pid=13850)[0m Epoch:[79/100] Train loss: 0.0065782561399085355, Recon loss: 0.0065761455646080325, KL Divergence: 5.866177267122269
[36m(ClientAppActor pid=13851)[0m Epoch:[81/100] Train loss: 0.006452480902949355, Recon loss: 0.006450347456812688, KL Divergence: 5.882581712198258
[36m(ClientAppActor pid=13850)[0m Epoch:[80/100] Train loss: 0.006573644645872991, Recon loss: 0.006571508133233965, KL Divergence: 5.863993516016007
[36m(ClientAppActor pid=13851)[0m Epoch:[82/100] Train loss: 0.006449543670024195, Recon loss: 0.006447408214453844, KL Divergence: 5.887879828906059
[36m(ClientAppActor pid=13850)[0m Epoch:[81/100] Train loss: 0.006571702398580146, Recon loss: 0.006569547268137649, KL Divergence: 5.871988608980179
[36m(ClientAppActor pid=13851)[0m Epoch:[83/100] Train loss: 0.006449078990183989, Recon loss: 0.006446923670243814, KL Divergence: 5.890106299924851
[36m(ClientAppActor pid=13850)[0m Epoch:[82/100] Train loss: 0.006570274019377666, Recon loss: 0.00656813325107396, KL Divergence: 5.875996192598343
[36m(ClientAppActor pid=13851)[0m Epoch:[84/100] Train loss: 0.006445632557084355, Recon loss: 0.006443486072067117, KL Divergence: 5.887821934819222
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 2 results and 8 failures
[93mWARNING [0m:   No fit_metrics_aggregation_fn provided
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 10 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 2]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 10 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 10 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 3]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 10 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 10 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 4]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 10 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 10 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 5]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 10 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 5fa0f93872c814bfc13e2e6001000000, name=ClientAppActor.__init__, pid=13851, memory used=0.47GB) was running was 7.44GB / 7.75GB (0.958896), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-68ad1ca72bb7740af5f1c1bb8010638db77a56438e22ab89cf5753c6*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
16476	0.98	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13526	0.97	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16714	0.81	ray::ClientAppActor.run
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13851	0.47	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 10.0.0.4, ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25) where the task (actor ID: 0e7da2ab9290d80ba29289fa01000000, name=ClientAppActor.__init__, pid=13850, memory used=0.68GB) was running was 7.50GB / 7.75GB (0.96685), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 188f885901b480568b3982d829874f6245654bf99555209adc03ed1e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.0.4`. To see the logs of the worker, use `ray logs worker-188f885901b480568b3982d829874f6245654bf99555209adc03ed1e*out -ip 10.0.0.4. Top 10 memory users:
PID	MEM(GB)	COMMAND
13526	1.53	python3 main.py --train --clear --elements 10000 --clients 10 --rounds 5 --epochs 100
16476	1.33	python3 main.py --train --clear --elements 10000 --clients 5 --rounds 5 --epochs 100
13850	0.68	ray::ClientAppActor.run
16713	0.66	ray::ClientAppActor.run
13657	0.09	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
13730	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13656	0.08	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
16599	0.07	/anaconda/envs/py38_default/bin/python3 -u /anaconda/envs/py38_default/lib/python3.10/site-packages/...
13627	0.06	/anaconda/envs/py38_default/lib/python3.10/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/...
16538	0.06	/anaconda/envs/py38_default/bin/python3 /anaconda/envs/py38_default/lib/python3.10/site-packages/ray...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 10 failures
[92mINFO [0m:      
[92mINFO [0m:      [SUMMARY]
[92mINFO [0m:      Run finished 5 round(s) in 29790.74s
[92mINFO [0m:      
/home/azureuser/FL/vae.py:181: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  latent_rep = np.array(latent_rep)
/home/azureuser/FL/vae.py:181: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  latent_rep = np.array(latent_rep)
[33m(raylet)[0m [2024-10-11 07:36:42,372 E 13721 13721] (raylet) node_manager.cc:2967: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: eae9f1d8d514a9435887be7cd2784446fd959b7f134e20e692f5fa25, IP: 10.0.0.4) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.0.4`
[33m(raylet)[0m 
[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[36m(ClientAppActor pid=13850)[0m /home/azureuser/FL/vae.py:97: UserWarning: Using a target size (torch.Size([1, 10, 10])) that is different to the input size (torch.Size([10, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
[36m(ClientAppActor pid=13850)[0m   recon_loss = F.mse_loss(recon_x, x, reduction='mean')
[36m(ClientAppActor pid=13850)[0m Epoch:[83/100] Train loss: 0.006567730758911785, Recon loss: 0.00656557169940952, KL Divergence: 5.878068125629425
[36m(ClientAppActor pid=13851)[0m Epoch:[85/100] Train loss: 0.006444923058444328, Recon loss: 0.006442728429071576, KL Divergence: 5.877229448699951
[36m(ClientAppActor pid=13850)[0m Epoch:[84/100] Train loss: 0.006565260730717273, Recon loss: 0.00656308980405147, KL Divergence: 5.8613492319107054
[36m(ClientAppActor pid=13851)[0m Epoch:[86/100] Train loss: 0.0064428002318187285, Recon loss: 0.006440578780070609, KL Divergence: 5.899649398040771
[36m(ClientAppActor pid=13850)[0m Epoch:[85/100] Train loss: 0.006564514178885747, Recon loss: 0.00656232810496449, KL Divergence: 5.901721189403534
[36m(ClientAppActor pid=13851)[0m Epoch:[87/100] Train loss: 0.006439473012961571, Recon loss: 0.0064372143466482155, KL Divergence: 5.880737817525864
[36m(ClientAppActor pid=13850)[0m Epoch:[86/100] Train loss: 0.006562969247250294, Recon loss: 0.006560771295427003, KL Divergence: 5.884365548324585
[36m(ClientAppActor pid=13851)[0m Epoch:[88/100] Train loss: 0.006439306366167784, Recon loss: 0.00643704516934049, KL Divergence: 5.903461513710022
[36m(ClientAppActor pid=13850)[0m Epoch:[87/100] Train loss: 0.006558553919416408, Recon loss: 0.006556335147110076, KL Divergence: 5.892133702802658
[36m(ClientAppActor pid=13851)[0m Epoch:[89/100] Train loss: 0.006434399210285574, Recon loss: 0.006432100944265676, KL Divergence: 5.881625733184815
[36m(ClientAppActor pid=13850)[0m Epoch:[88/100] Train loss: 0.0065565631113138805, Recon loss: 0.0065543302339664475, KL Divergence: 5.90801182627678
[36m(ClientAppActor pid=13851)[0m Epoch:[90/100] Train loss: 0.006434664276047169, Recon loss: 0.006432309065177469, KL Divergence: 5.920549718022347
[36m(ClientAppActor pid=13851)[0m Epoch:[91/100] Train loss: 0.006432563263070824, Recon loss: 0.00643018870291944, KL Divergence: 5.89614983625412
[36m(ClientAppActor pid=13850)[0m Epoch:[89/100] Train loss: 0.006554782594527478, Recon loss: 0.006552533927857348, KL Divergence: 5.911671505832672
[36m(ClientAppActor pid=13851)[0m Epoch:[92/100] Train loss: 0.006429121359642795, Recon loss: 0.0064267444997566595, KL Divergence: 5.922408636569977
[36m(ClientAppActor pid=13850)[0m Epoch:[90/100] Train loss: 0.006552147920394145, Recon loss: 0.00654986544004887, KL Divergence: 5.89648119096756
[36m(ClientAppActor pid=13851)[0m Epoch:[93/100] Train loss: 0.006427946045652516, Recon loss: 0.006425519205321234, KL Divergence: 5.898166710734367
[36m(ClientAppActor pid=13850)[0m Epoch:[91/100] Train loss: 0.006550517052055056, Recon loss: 0.006548231751578442, KL Divergence: 5.926233276224137
[36m(ClientAppActor pid=13851)[0m Epoch:[94/100] Train loss: 0.00642644789200167, Recon loss: 0.0064240207840820405, KL Divergence: 5.939629949140548
[36m(ClientAppActor pid=13850)[0m Epoch:[92/100] Train loss: 0.006548627059191313, Recon loss: 0.006546347116745164, KL Divergence: 5.925141239595413
[36m(ClientAppActor pid=13851)[0m Epoch:[95/100] Train loss: 0.006424341497827845, Recon loss: 0.006421892832294361, KL Divergence: 5.930164354276657
[36m(ClientAppActor pid=13850)[0m Epoch:[93/100] Train loss: 0.0065472592132893625, Recon loss: 0.006544977965593989, KL Divergence: 5.943535863208771
[36m(ClientAppActor pid=13851)[0m Epoch:[96/100] Train loss: 0.006421863482975368, Recon loss: 0.0064193677978725645, KL Divergence: 5.93410872964859
[36m(ClientAppActor pid=13850)[0m Epoch:[94/100] Train loss: 0.006543998406832725, Recon loss: 0.006541715410829329, KL Divergence: 5.921601239585876
[36m(ClientAppActor pid=13851)[0m Epoch:[97/100] Train loss: 0.006422414586722243, Recon loss: 0.006419909728288712, KL Divergence: 5.938764174938202
[36m(ClientAppActor pid=13850)[0m Epoch:[95/100] Train loss: 0.006543316088536813, Recon loss: 0.006541019163485362, KL Divergence: 5.925729384231567
[36m(ClientAppActor pid=13851)[0m Epoch:[98/100] Train loss: 0.006417885700156694, Recon loss: 0.006415374553271613, KL Divergence: 5.93746692006588
[36m(ClientAppActor pid=13850)[0m Epoch:[96/100] Train loss: 0.006540266282055745, Recon loss: 0.00653795181825326, KL Divergence: 5.942580159950256
[36m(ClientAppActor pid=13851)[0m Epoch:[99/100] Train loss: 0.006418085980208343, Recon loss: 0.006415538920136623, KL Divergence: 5.9449582924366
[36m(ClientAppActor pid=13850)[0m Epoch:[97/100] Train loss: 0.00653822443357376, Recon loss: 0.006535883376154106, KL Divergence: 5.938166707277298
[36m(ClientAppActor pid=13851)[0m Epoch:[100/100] Train loss: 0.006417582262315045, Recon loss: 0.00641503275286268, KL Divergence: 5.941400149917603
[36m(ClientAppActor pid=13850)[0m Epoch:[98/100] Train loss: 0.006536371598111964, Recon loss: 0.0065340130446491455, KL Divergence: 5.945255989694595
[36m(ClientAppActor pid=13851)[0m Epoch:[1/100] Train loss: 0.03574913558230037, Recon loss: 0.03499744846946451, KL Divergence: 3.6867913339089604
[36m(ClientAppActor pid=13850)[0m Epoch:[99/100] Train loss: 0.006534093603159544, Recon loss: 0.006531762902321316, KL Divergence: 5.9456198847293855
[36m(ClientAppActor pid=13851)[0m Epoch:[2/100] Train loss: 0.009392789020980672, Recon loss: 0.009372124893385807, KL Divergence: 5.118002028298378
[36m(ClientAppActor pid=13850)[0m Epoch:[100/100] Train loss: 0.006533759087449562, Recon loss: 0.006531363945677276, KL Divergence: 5.955400122761726
[36m(ClientAppActor pid=13851)[0m Epoch:[3/100] Train loss: 0.008194067378185718, Recon loss: 0.008170036322418674, KL Divergence: 5.264800185275078
[36m(ClientAppActor pid=13850)[0m Epoch:[1/100] Train loss: 0.0388092003019512, Recon loss: 0.03802639540079763, KL Divergence: 3.9399628762081265
[36m(ClientAppActor pid=13851)[0m Epoch:[4/100] Train loss: 0.00793379641196043, Recon loss: 0.007911139640852343, KL Divergence: 5.252664658546448
[36m(ClientAppActor pid=13850)[0m Epoch:[2/100] Train loss: 0.010012577579840764, Recon loss: 0.009974761504786874, KL Divergence: 5.490559370970726
[36m(ClientAppActor pid=13851)[0m Epoch:[5/100] Train loss: 0.007735898738735886, Recon loss: 0.007715063101098349, KL Divergence: 5.318980006074906
[36m(ClientAppActor pid=13850)[0m Epoch:[3/100] Train loss: 0.00858204216034901, Recon loss: 0.00854183875187191, KL Divergence: 5.36899845571518
[36m(ClientAppActor pid=13851)[0m Epoch:[6/100] Train loss: 0.007586738991542552, Recon loss: 0.00756903049723951, KL Divergence: 5.387955434846878
[36m(ClientAppActor pid=13850)[0m Epoch:[4/100] Train loss: 0.008228388878138867, Recon loss: 0.008190338084280666, KL Divergence: 5.377558270812035
[36m(ClientAppActor pid=13851)[0m Epoch:[7/100] Train loss: 0.007485643574437199, Recon loss: 0.0074703795438463205, KL Divergence: 5.415950717186928
[36m(ClientAppActor pid=13850)[0m Epoch:[5/100] Train loss: 0.008037733065330031, Recon loss: 0.008002500448296632, KL Divergence: 5.384805352449417
[36m(ClientAppActor pid=13851)[0m Epoch:[8/100] Train loss: 0.0074055067147421145, Recon loss: 0.007392196373666593, KL Divergence: 5.439007270646095
[36m(ClientAppActor pid=13850)[0m Epoch:[6/100] Train loss: 0.00789769293432837, Recon loss: 0.007866013798064887, KL Divergence: 5.4390429926157
[36m(ClientAppActor pid=13851)[0m Epoch:[9/100] Train loss: 0.007337472082084787, Recon loss: 0.00732580862764753, KL Divergence: 5.442801566576958
[36m(ClientAppActor pid=13850)[0m Epoch:[7/100] Train loss: 0.00778153048565091, Recon loss: 0.007752661141148564, KL Divergence: 5.444570686650276
[36m(ClientAppActor pid=13851)[0m Epoch:[10/100] Train loss: 0.007265340024599936, Recon loss: 0.0072547994089230995, KL Divergence: 5.424618982791901
[36m(ClientAppActor pid=13850)[0m Epoch:[8/100] Train loss: 0.007699242206025338, Recon loss: 0.00767255189453299, KL Divergence: 5.44996163597107
[36m(ClientAppActor pid=13851)[0m Epoch:[11/100] Train loss: 0.0072393919894471765, Recon loss: 0.007229863725712857, KL Divergence: 5.430421170711518
Saving round 1 aggregated_parameters...
Traceback (most recent call last):
  File "/home/azureuser/FL/main.py", line 365, in <module>
    latent_rep = rc.fit_transform(latent_rep)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/sklearn/utils/_set_output.py", line 313, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/sklearn/base.py", line 1098, in fit_transform
    return self.fit(X, **fit_params).transform(X)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/sklearn/preprocessing/_data.py", line 878, in fit
    return self.partial_fit(X, y, sample_weight)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/sklearn/base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/sklearn/preprocessing/_data.py", line 914, in partial_fit
    X = self._validate_data(
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/sklearn/base.py", line 633, in _validate_data
    out = check_array(X, input_name="X", **check_params)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1012, in check_array
    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)
  File "/anaconda/envs/py38_default/lib/python3.10/site-packages/sklearn/utils/_array_api.py", line 751, in _asarray_with_order
    array = numpy.asarray(array, order=order, dtype=dtype)
ValueError: only one element tensors can be converted to Python scalars
